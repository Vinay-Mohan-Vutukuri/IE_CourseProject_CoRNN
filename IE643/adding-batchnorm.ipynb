{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-27T17:02:16.884761Z","iopub.execute_input":"2023-11-27T17:02:16.886117Z","iopub.status.idle":"2023-11-27T17:02:17.246591Z","shell.execute_reply.started":"2023-11-27T17:02:16.886066Z","shell.execute_reply":"2023-11-27T17:02:17.245789Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef get_batch(T,batch_size):\n    values = torch.rand(T, batch_size, requires_grad=False)\n    indices = torch.zeros_like(values)\n    half = int(T / 2)\n    for i in range(batch_size):\n        half_1 = np.random.randint(half)\n        hals_2 = np.random.randint(half, T)\n        indices[half_1, i] = 1\n        indices[hals_2, i] = 1\n\n    data = torch.stack((values, indices), dim=-1)\n    targets = torch.mul(values, indices).sum(dim=0)\n    return data, targets","metadata":{"execution":{"iopub.status.busy":"2023-11-27T17:02:17.248396Z","iopub.execute_input":"2023-11-27T17:02:17.248893Z","iopub.status.idle":"2023-11-27T17:02:19.968935Z","shell.execute_reply.started":"2023-11-27T17:02:17.248859Z","shell.execute_reply":"2023-11-27T17:02:19.967953Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nimport torch\nfrom torch.autograd import Variable\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass coRNNCell(nn.Module):\n    def __init__(self, n_inp, n_hid, dt, gamma, epsilon):\n        super(coRNNCell, self).__init__()\n        self.dt = dt\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.i2h = nn.Linear(n_inp + n_hid + n_hid, n_hid)\n        self.bn = nn.BatchNorm1d(n_hid) \n\n    def forward(self,x,hy,hz):\n        hz = hz + self.dt * (torch.tanh(self.bn(self.i2h(torch.cat((x, hz, hy),1))))\n                                   - self.gamma * hy - self.epsilon * hz)\n        hy = hy + self.dt * hz\n\n        return hy, hz\n\nclass coRNN(nn.Module):\n    def __init__(self, n_inp, n_hid, n_out, dt, gamma, epsilon):\n        super(coRNN, self).__init__()\n        self.n_hid = n_hid\n        self.cell = coRNNCell(n_inp,n_hid,dt,gamma,epsilon)\n        self.readout = nn.Linear(n_hid, n_out)\n        self.bn = nn.BatchNorm1d(n_hid) \n\n    def forward(self, x):\n        ## initialize hidden states\n        hy = Variable(torch.zeros(x.size(1),self.n_hid)).to(device)\n        hz = Variable(torch.zeros(x.size(1),self.n_hid)).to(device)\n\n        for t in range(x.size(0)):\n            hy, hz = self.cell(x[t],hy,hz)\n        output = self.readout(self.bn(hy))\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-11-27T17:02:19.970185Z","iopub.execute_input":"2023-11-27T17:02:19.970581Z","iopub.status.idle":"2023-11-27T17:02:20.004543Z","shell.execute_reply.started":"2023-11-27T17:02:19.970555Z","shell.execute_reply":"2023-11-27T17:02:20.003555Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from torch import nn, optim\nimport torch\n# import model\nimport torch.nn.utils\n# import utils\n# import argparse\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n# parser = argparse.ArgumentParser(description='training parameters')\n\n# parser.add_argument('--n_hid', type=int, default=128,\n#                     help='hidden size of recurrent net')\n# parser.add_argument('--T', type=int, default=100,\n#                     help='length of sequences')\n# parser.add_argument('--max_steps', type=int, default=60000,\n#                     help='max learning steps')\n# parser.add_argument('--log_interval', type=int, default=100,\n#                     help='log interval')\n# parser.add_argument('--batch', type=int, default=50,\n#                     help='batch size')\n# parser.add_argument('--batch_test', type=int, default=1000,\n#                     help='size of test set')\n# parser.add_argument('--lr', type=float, default=2e-2,\n#                     help='learning rate')\n# parser.add_argument('--dt',type=float, default=6e-2,\n#                     help='step size <dt> of the coRNN')\n# parser.add_argument('--gamma',type=float, default=66,\n#                     help='y controle parameter <gamma> of the coRNN')\n# parser.add_argument('--epsilon',type=float, default = 15,\n#                     help='z controle parameter <epsilon> of the coRNN')\n\n# args = parser.parse_args()\nargs = {\"n_hid\":128,\n      \"T\" : 100,\n      \"max_steps\":30000,\n      \"log_interval\":100,\n      \"batch\":50,\n      \"batch_test\":1000,\n      \"lr\":2e-2,\n      \"dt\":6e-2,\n      \"gamma\":66,\n      \"epsilon\":15}\n\nn_inp = 2\nn_out = 1\n\nmodel = coRNN(n_inp, args[\"n_hid\"], n_out, args[\"dt\"], args[\"gamma\"], args[\"epsilon\"]).to(device)\n\n\nobjective = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=args[\"lr\"])\ntest_mse_500 = []\ntest_mse_2000 = []\ntest_mse_5000 = []\n\ndef test():\n    model.eval()\n    with torch.no_grad():\n        data, label = get_batch(args[\"T\"], args[\"batch_test\"])\n        label = label.unsqueeze(1)\n        out = model(data.to(device))\n        loss = objective(out, label.to(device))\n\n    return loss.item()\n\ndef train(x,l):\n    \n    \n    for i in range(args[\"max_steps\"]):\n#         model.train()\n        data, label = get_batch(x,args[\"batch\"])\n#         print(data.shape)\n#         print(label.shape)\n        label = label.unsqueeze(1)\n#         print(label.shape)\n\n        optimizer.zero_grad()\n        out = model(data.to(device))\n        loss = objective(out, label.to(device))\n        loss.backward()\n        optimizer.step()\n        if(i%100==0 and i!=0):\n            mse_error = test()\n            print('Test MSE: {:.6f}'.format(mse_error))\n            l.append(mse_error)\n            model.train()\n            \n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T17:02:20.006954Z","iopub.execute_input":"2023-11-27T17:02:20.007346Z","iopub.status.idle":"2023-11-27T17:02:23.049834Z","shell.execute_reply.started":"2023-11-27T17:02:20.007313Z","shell.execute_reply":"2023-11-27T17:02:23.049046Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train(100,test_mse_500)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T17:02:23.050859Z","iopub.execute_input":"2023-11-27T17:02:23.051134Z","iopub.status.idle":"2023-11-27T17:30:51.379973Z","shell.execute_reply.started":"2023-11-27T17:02:23.051111Z","shell.execute_reply":"2023-11-27T17:30:51.379198Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Test MSE: 0.155186\nTest MSE: 0.176128\nTest MSE: 0.169130\nTest MSE: 0.193527\nTest MSE: 0.165970\nTest MSE: 0.184066\nTest MSE: 0.186148\nTest MSE: 0.158288\nTest MSE: 0.172829\nTest MSE: 0.180175\nTest MSE: 0.171954\nTest MSE: 0.169865\nTest MSE: 0.168531\nTest MSE: 0.182099\nTest MSE: 0.173234\nTest MSE: 0.170290\nTest MSE: 0.167101\nTest MSE: 0.173063\nTest MSE: 0.163827\nTest MSE: 0.163511\nTest MSE: 0.170801\nTest MSE: 0.168119\nTest MSE: 0.167162\nTest MSE: 0.172298\nTest MSE: 0.168527\nTest MSE: 0.159989\nTest MSE: 0.163508\nTest MSE: 0.167529\nTest MSE: 0.167778\nTest MSE: 0.157105\nTest MSE: 0.167005\nTest MSE: 0.161474\nTest MSE: 0.152374\nTest MSE: 0.149399\nTest MSE: 0.167803\nTest MSE: 0.159183\nTest MSE: 0.150765\nTest MSE: 0.154293\nTest MSE: 0.158895\nTest MSE: 0.150819\nTest MSE: 0.154529\nTest MSE: 0.158936\nTest MSE: 0.156776\nTest MSE: 0.157708\nTest MSE: 0.165969\nTest MSE: 0.167639\nTest MSE: 0.151868\nTest MSE: 0.156999\nTest MSE: 0.165298\nTest MSE: 0.170136\nTest MSE: 0.156777\nTest MSE: 0.159766\nTest MSE: 0.150040\nTest MSE: 0.145719\nTest MSE: 0.166405\nTest MSE: 0.148687\nTest MSE: 0.154452\nTest MSE: 0.160043\nTest MSE: 0.158033\nTest MSE: 0.167362\nTest MSE: 0.156142\nTest MSE: 0.156120\nTest MSE: 0.166106\nTest MSE: 0.160374\nTest MSE: 0.159387\nTest MSE: 0.164819\nTest MSE: 0.158927\nTest MSE: 0.185697\nTest MSE: 0.182031\nTest MSE: 0.158017\nTest MSE: 0.163878\nTest MSE: 0.166310\nTest MSE: 0.149791\nTest MSE: 0.165821\nTest MSE: 0.153531\nTest MSE: 0.179685\nTest MSE: 0.160548\nTest MSE: 0.150630\nTest MSE: 0.166876\nTest MSE: 0.148063\nTest MSE: 0.153735\nTest MSE: 0.163951\nTest MSE: 0.154565\nTest MSE: 0.160167\nTest MSE: 0.157666\nTest MSE: 0.163138\nTest MSE: 0.144794\nTest MSE: 0.164482\nTest MSE: 0.158206\nTest MSE: 0.154340\nTest MSE: 0.172409\nTest MSE: 0.146061\nTest MSE: 0.149999\nTest MSE: 0.156599\nTest MSE: 0.158791\nTest MSE: 0.161307\nTest MSE: 0.169229\nTest MSE: 0.171189\nTest MSE: 0.162917\nTest MSE: 0.174481\nTest MSE: 0.160701\nTest MSE: 0.167175\nTest MSE: 0.181719\nTest MSE: 0.169951\nTest MSE: 0.164176\nTest MSE: 0.172090\nTest MSE: 0.155404\nTest MSE: 0.182353\nTest MSE: 0.159044\nTest MSE: 0.165056\nTest MSE: 0.159091\nTest MSE: 0.171645\nTest MSE: 0.164923\nTest MSE: 0.159986\nTest MSE: 0.158258\nTest MSE: 0.152322\nTest MSE: 0.156996\nTest MSE: 0.165850\nTest MSE: 0.158874\nTest MSE: 0.155850\nTest MSE: 0.169335\nTest MSE: 0.153317\nTest MSE: 0.158765\nTest MSE: 0.159152\nTest MSE: 0.161191\nTest MSE: 0.166260\nTest MSE: 0.158987\nTest MSE: 0.153320\nTest MSE: 0.158212\nTest MSE: 0.163626\nTest MSE: 0.153271\nTest MSE: 0.156499\nTest MSE: 0.163131\nTest MSE: 0.162419\nTest MSE: 0.165267\nTest MSE: 0.161521\nTest MSE: 0.150773\nTest MSE: 0.153699\nTest MSE: 0.173945\nTest MSE: 0.152503\nTest MSE: 0.151931\nTest MSE: 0.158099\nTest MSE: 0.175962\nTest MSE: 0.170534\nTest MSE: 0.161635\nTest MSE: 0.169316\nTest MSE: 0.156399\nTest MSE: 0.175854\nTest MSE: 0.164019\nTest MSE: 0.166025\nTest MSE: 0.161937\nTest MSE: 0.161402\nTest MSE: 0.156541\nTest MSE: 0.152140\nTest MSE: 0.164922\nTest MSE: 0.162980\nTest MSE: 0.164904\nTest MSE: 0.171998\nTest MSE: 0.172316\nTest MSE: 0.174055\nTest MSE: 0.162715\nTest MSE: 0.157093\nTest MSE: 0.162076\nTest MSE: 0.171317\nTest MSE: 0.152367\nTest MSE: 0.154358\nTest MSE: 0.156213\nTest MSE: 0.152933\nTest MSE: 0.159429\nTest MSE: 0.160553\nTest MSE: 0.162839\nTest MSE: 0.165939\nTest MSE: 0.151406\nTest MSE: 0.150710\nTest MSE: 0.147526\nTest MSE: 0.157211\nTest MSE: 0.163923\nTest MSE: 0.163370\nTest MSE: 0.172761\nTest MSE: 0.169651\nTest MSE: 0.159299\nTest MSE: 0.161115\nTest MSE: 0.147537\nTest MSE: 0.156871\nTest MSE: 0.162328\nTest MSE: 0.149950\nTest MSE: 0.153149\nTest MSE: 0.066200\nTest MSE: 0.090446\nTest MSE: 0.317080\nTest MSE: 0.132641\nTest MSE: 0.239765\nTest MSE: 0.088520\nTest MSE: 0.062570\nTest MSE: 0.190768\nTest MSE: 0.090598\nTest MSE: 0.103113\nTest MSE: 0.053720\nTest MSE: 0.101410\nTest MSE: 0.030563\nTest MSE: 0.029156\nTest MSE: 0.079378\nTest MSE: 0.043094\nTest MSE: 0.053729\nTest MSE: 0.032565\nTest MSE: 0.119017\nTest MSE: 0.023699\nTest MSE: 0.018421\nTest MSE: 0.090564\nTest MSE: 0.018530\nTest MSE: 0.024967\nTest MSE: 0.017943\nTest MSE: 0.017911\nTest MSE: 0.027781\nTest MSE: 0.266196\nTest MSE: 0.027810\nTest MSE: 0.017086\nTest MSE: 0.067729\nTest MSE: 0.075181\nTest MSE: 0.150977\nTest MSE: 0.023407\nTest MSE: 0.153239\nTest MSE: 0.028265\nTest MSE: 0.203525\nTest MSE: 0.057368\nTest MSE: 0.087744\nTest MSE: 0.178337\nTest MSE: 0.087555\nTest MSE: 0.217712\nTest MSE: 0.013364\nTest MSE: 0.284774\nTest MSE: 0.018219\nTest MSE: 0.101039\nTest MSE: 0.157679\nTest MSE: 0.074541\nTest MSE: 0.011820\nTest MSE: 0.036954\nTest MSE: 0.033189\nTest MSE: 0.113775\nTest MSE: 0.018193\nTest MSE: 0.031362\nTest MSE: 0.059370\nTest MSE: 0.042082\nTest MSE: 0.097499\nTest MSE: 0.040963\nTest MSE: 0.122053\nTest MSE: 0.010470\nTest MSE: 0.019657\nTest MSE: 0.037784\nTest MSE: 0.014377\nTest MSE: 0.009965\nTest MSE: 0.150411\nTest MSE: 0.046807\nTest MSE: 0.023647\nTest MSE: 0.012510\nTest MSE: 0.032882\nTest MSE: 0.010892\nTest MSE: 0.015562\nTest MSE: 0.128878\nTest MSE: 0.014465\nTest MSE: 0.023712\nTest MSE: 0.053885\nTest MSE: 0.140583\nTest MSE: 0.179247\nTest MSE: 0.016577\nTest MSE: 0.015975\nTest MSE: 0.084179\nTest MSE: 0.031211\nTest MSE: 0.035844\nTest MSE: 0.023385\nTest MSE: 0.013193\nTest MSE: 0.017527\nTest MSE: 0.055290\nTest MSE: 0.025359\nTest MSE: 0.023481\nTest MSE: 0.118046\nTest MSE: 0.013592\nTest MSE: 0.023523\nTest MSE: 0.096893\nTest MSE: 0.009201\nTest MSE: 0.012568\nTest MSE: 0.019998\nTest MSE: 0.019525\nTest MSE: 0.015160\nTest MSE: 0.010716\nTest MSE: 0.009615\nTest MSE: 0.010526\nTest MSE: 0.005685\nTest MSE: 0.017699\nTest MSE: 0.039631\nTest MSE: 0.041958\nTest MSE: 0.015637\nTest MSE: 0.099302\nTest MSE: 0.020304\nTest MSE: 0.077661\nTest MSE: 0.024899\nTest MSE: 0.075000\nTest MSE: 0.023935\nTest MSE: 0.010816\n","output_type":"stream"}]}]}