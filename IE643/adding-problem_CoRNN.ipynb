{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef get_batch(T,batch_size):\n    values = torch.rand(T, batch_size, requires_grad=False)\n    indices = torch.zeros_like(values)\n    half = int(T / 2)\n    for i in range(batch_size):\n        half_1 = np.random.randint(half)\n        hals_2 = np.random.randint(half, T)\n        indices[half_1, i] = 1\n        indices[hals_2, i] = 1\n\n    data = torch.stack((values, indices), dim=-1)\n    targets = torch.mul(values, indices).sum(dim=0)\n    return data, targets","metadata":{"execution":{"iopub.status.busy":"2023-10-13T11:39:20.148308Z","iopub.execute_input":"2023-10-13T11:39:20.148652Z","iopub.status.idle":"2023-10-13T11:39:25.186560Z","shell.execute_reply.started":"2023-10-13T11:39:20.148623Z","shell.execute_reply":"2023-10-13T11:39:25.185622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nimport torch\nfrom torch.autograd import Variable\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass coRNNCell(nn.Module):\n    def __init__(self, n_inp, n_hid, dt, gamma, epsilon):\n        super(coRNNCell, self).__init__()\n        self.dt = dt\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.i2h = nn.Linear(n_inp + n_hid + n_hid, n_hid)\n\n    def forward(self,x,hy,hz):\n        hz = hz + self.dt * (torch.tanh(self.i2h(torch.cat((x, hz, hy),1)))\n                                   - self.gamma * hy - self.epsilon * hz)\n        hy = hy + self.dt * hz\n\n        return hy, hz\n\nclass coRNN(nn.Module):\n    def __init__(self, n_inp, n_hid, n_out, dt, gamma, epsilon):\n        super(coRNN, self).__init__()\n        self.n_hid = n_hid\n        self.cell = coRNNCell(n_inp,n_hid,dt,gamma,epsilon)\n        self.readout = nn.Linear(n_hid, n_out)\n\n    def forward(self, x):\n        ## initialize hidden states\n        hy = Variable(torch.zeros(x.size(1),self.n_hid)).to(device)\n        hz = Variable(torch.zeros(x.size(1),self.n_hid)).to(device)\n\n        for t in range(x.size(0)):\n            hy, hz = self.cell(x[t],hy,hz)\n        output = self.readout(hy)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-10-13T11:39:57.198234Z","iopub.execute_input":"2023-10-13T11:39:57.198632Z","iopub.status.idle":"2023-10-13T11:39:57.207226Z","shell.execute_reply.started":"2023-10-13T11:39:57.198578Z","shell.execute_reply":"2023-10-13T11:39:57.206223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn, optim\nimport torch\n# import model\nimport torch.nn.utils\n# import utils\n# import argparse\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n# parser = argparse.ArgumentParser(description='training parameters')\n\n# parser.add_argument('--n_hid', type=int, default=128,\n#                     help='hidden size of recurrent net')\n# parser.add_argument('--T', type=int, default=100,\n#                     help='length of sequences')\n# parser.add_argument('--max_steps', type=int, default=60000,\n#                     help='max learning steps')\n# parser.add_argument('--log_interval', type=int, default=100,\n#                     help='log interval')\n# parser.add_argument('--batch', type=int, default=50,\n#                     help='batch size')\n# parser.add_argument('--batch_test', type=int, default=1000,\n#                     help='size of test set')\n# parser.add_argument('--lr', type=float, default=2e-2,\n#                     help='learning rate')\n# parser.add_argument('--dt',type=float, default=6e-2,\n#                     help='step size <dt> of the coRNN')\n# parser.add_argument('--gamma',type=float, default=66,\n#                     help='y controle parameter <gamma> of the coRNN')\n# parser.add_argument('--epsilon',type=float, default = 15,\n#                     help='z controle parameter <epsilon> of the coRNN')\n\n# args = parser.parse_args()\nargs = {\"n_hid\":128,\n      \"T\" : 100,\n      \"max_steps\":60000,\n      \"log_interval\":100,\n      \"batch\":50,\n      \"batch_test\":1000,\n      \"lr\":2e-2,\n      \"dt\":6e-2,\n      \"gamma\":66,\n      \"epsilon\":15}\n\nn_inp = 2\nn_out = 1\n\nmodel = coRNN(n_inp, args[\"n_hid\"], n_out, args[\"dt\"], args[\"gamma\"], args[\"epsilon\"]).to(device)\n\n\nobjective = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=args[\"lr\"])\n\ndef test():\n    model.eval()\n    with torch.no_grad():\n        data, label = get_batch(args[\"T\"], args[\"batch_test\"])\n        label = label.unsqueeze(1)\n        out = model(data.to(device))\n        loss = objective(out, label.to(device))\n\n    return loss.item()\n\ndef train():\n    test_mse = []\n    for i in range(args[\"max_steps\"]):\n        data, label = get_batch(args[\"T\"],args[\"batch\"])\n        label = label.unsqueeze(1)\n\n        optimizer.zero_grad()\n        out = model(data.to(device))\n        loss = objective(out, label.to(device))\n        loss.backward()\n        optimizer.step()\n\n        if(i%100==0 and i!=0):\n            mse_error = test()\n            print('Test MSE: {:.6f}'.format(mse_error))\n            test_mse.append(mse_error)\n            model.train()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-13T11:49:28.265238Z","iopub.execute_input":"2023-10-13T11:49:28.265568Z","iopub.status.idle":"2023-10-13T11:49:33.017909Z","shell.execute_reply.started":"2023-10-13T11:49:28.265540Z","shell.execute_reply":"2023-10-13T11:49:33.016882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train()","metadata":{"execution":{"iopub.status.busy":"2023-10-13T11:49:48.641403Z","iopub.execute_input":"2023-10-13T11:49:48.641727Z","iopub.status.idle":"2023-10-13T11:52:06.144728Z","shell.execute_reply.started":"2023-10-13T11:49:48.641699Z","shell.execute_reply":"2023-10-13T11:52:06.143455Z"},"trusted":true},"execution_count":null,"outputs":[]}]}